{
  "metadata": {
    "name": "made-lesson-6-spark-df",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Made. Spark DataFrame API\n\nНа этом семинаре мы разберем основные функции Spark DataFrame API и попробуем их применить на синтетическом примере анализа результатов A/B-теста.\n\nМы работаем в среде Apache Zeppelin. В этой среде по умолчанию создается сессия спарка и сделан импорт синтаксического сахара для удобства работы со спарком."
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "spark"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Однако, если мы будем работать в других средах, например, собирать jar с заданием и запускать его через `spark-submit`, то нам понадобится в явном виде создать сессию спарка.\n\nДля этого мы можем импортировать зависимость Spark SQL API (Spark DF API является частью его)."
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql._"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Теперь мы можем создать сессию. При создании сессии нам может быть необходимо передать различные параметры внутрь нее.\n\nВсе доступные параметры спарка можно посмотреть [здесь](https://spark.apache.org/docs/latest/configuration.html)."
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val spark \u003d SparkSession.builder()\r\n    // адрес мастера\r\n    .master(\"local[*]\")\r\n    // имя приложения в интерфейсе спарка\r\n    .appName(\"made-demo\")\r\n//     .config(\"spark.executor.memory\",  \"2g\")\r\n//     .config(\"spark.executor.cores\", \"2\")\r\n//     .config(\"spark.driver.memory\", \"2g\")\r\n    .getOrCreate()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "После создания сессии мы можем импортировать синтаксический сахар"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import spark.implicits._"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Создание DataFrame"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val names \u003d Seq(\"Vanya\", \"Petya\", \"Vasya\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Из коллекций функцией .toDF (нужен импорт spark.implicits._)"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df \u003d names.toDF(\"name\")\r\ndf.show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Коллекция кейс классов"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "case class Person(name: String)\r\n// List((\"Vanya\", 25, \"Moscow\"))\r\nval df \u003d spark.createDataFrame(names.map(Person))\r\ndf.show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "RDD[Row] и Schema"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.types._\r\n\r\nval schema \u003d StructType(Seq(\r\n    StructField(\"name\", StringType)\r\n))"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val rdd \u003d spark\r\n    .sparkContext\r\n    .parallelize(names.map(x \u003d\u003e Row(x)))\r\n\r\nval df \u003d spark.createDataFrame(rdd, schema)\r\ndf.show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "RDD от кейс классов"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val rdd \u003d spark\r\n    .sparkContext\r\n    .parallelize(names.map(x \u003d\u003e Person(x)))\r\n\r\nval df \u003d spark.createDataFrame(rdd)\r\ndf.show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Чтением файла."
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df \u003d spark.read\r\n    .option(\"header\", \"true\")\r\n    .option(\"inferSchema\", \"true\")\r\n    .option(\"sep\", \",\")\r\n    .csv(\"/notebook/names.csv\")\r\n\r\ndf.show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Передавать параметры можно единой мапой"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val options \u003d Seq(\r\n    \"header\" -\u003e \"true\",\r\n    \"inferSchema\" -\u003e \"true\"\r\n).toMap\r\n\r\nval df \u003d spark.read\r\n    .options(options)\r\n    .csv(\"/notebook/names.csv\")\r\n\r\ndf.show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Базовые функции\n\nВ этом и дальнейшем разделах мы будем использовать симулированные данные о реакциях пользователя на рекомендательные системы.\n\nДанные о реакциях собраны в рамках аб теста между двумя моделями\n\n1. `default` -- модель, которая выдает ровно одно предсказание для пользователя перед началом аб-теста. То есть в этой группе конкретному пользователю всегда показывается один и тот же баннер\n2. `model` -- модель, которая учится в онлайн (контекстный бандит). Предполагается, что со временем она должна выучивать более подходящие пользователям рекоммендации.\n\nВ рамках этого ноутбука хотим построить отчет, чтобы увидеть\n\n1. Правда ли `model` работает в среднем лучше, чем `default`?\n2. Улучшается ли со временем результат работы `model`?\n3. Есть ли какой-то период времени, когда `model` работает хуже `default`?\n\n\nКроме того, нужно учитывать тот факт, что в рамках одной сессии пользователь может несколько раз увидеть один и тот же баннер / кликнуть на него несколько раз. Дубли событий лучше удалить, чтобы избежать лишнего шума в итоговой метрике.\n\n\nДля начала нам необходимо прочитать данные"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df \u003d spark.read\r\n    .option(\"header\", \"true\")\r\n    .option(\"inferSchema\", \"true\")\r\n    .option(\"sep\", \",\")\r\n    .csv(\"/notebook/ab-test-sim-both.csv\")\r\n\r\ndf.show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Превью данных можно сделать в разном виде\n\n1. Выбрать сколько строк показать\n2. Ограничить количество символов в колонке\n3. Показать данные вертикально (проще читать, когда много колонок в таблице)"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df.show(1) // top 1 row\n\ndf.show(1, 3) // truncate strings to given number\n\ndf.show(3, 3, true) // print vertical"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Посмотрим сколько всего наблюдений в таблице"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df.count"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Давайте посмотрим, что есть в данных, выведя схему. В данном случае мы можем увидеть ее и из `show`, но так как `show` -- операция действия, то каждый раз ее лучше не использовать, схема может вычисляться по метаданным без рассчета всего датасета. Кроме того, данные могут иметь сложную вложенную структуру, что будет удобно отражено в схеме."
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df.printSchema"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Как видно из схемы, у нас есть пять полей\n\n1. `user_id` -- идентификатор пользователя\n2. `item_id` -- идентификатор объекта (баннера)\n3. `event` -- событие, которое совершил пользователь, может быть одним из четырех типов\n    1. `SHOW` -- означает, что баннер отрендерился на экране пользователя\n    2. `CLICK` -- пользователь перешел по баннеру (позитивно отреагировал)\n    3. `OPEN` -- пользователь перешел по баннеру (позитивно отреагировал). По факту совпадает с событием `CLICK`, поэтому чуть позже мы его поменяем в данных на `CLICK`.\n    4. `HIDE` -- пользователь скрыл баннер (негативно отреагировал)\n4. Время клика в секундах\n5. Уверенность модели в рекомендации"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\r\n\r\nФункции над Spark DataFrames принимают на вход колонки.\r\n\r\nПосмотрим на примере фильтрации"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\ndf.filter(user_id \u003d\u003d\u003d 9771).show\ndf.filter($\"user_id\" \u003d\u003d\u003d 9771).show\ndf.filter(col(\"user_id\") \u003d\u003d\u003d 9771).show // предпочтительный вариант, ибо более явный, находится в org.apache.spark.sql.functions._ + проще переиспользовать между spark / pyspark"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df.filter(\"user_id \u003d 9771\").show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\nТакже можно зарегистрировать таблицу и пользоваться синтаксисом SQL"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df.registerTempTable(\"df\")\n\nspark.sql(\"\"\"\n    select *\n    from df\n    where user_id \u003d 9771\n\"\"\").show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Функции над колонками\n\nhttps://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html\n\n\nЕсть набор стандартных функций, все они принимают на вход колонку."
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val dfWithLength \u003d df\r\n    .withColumn(\"length\", length(col(\"event\"))) // take length of each element in column `event`\r\n    .withColumn(\"constant\", lit(1)) // create column from constant \r\ndfWithLength.show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Метод `select` позволяет выбрать необходимые нам колонки (или все колонки при `\"*\"`) и добавить новые. Не забывайте их переименовывать, иначе будут непонятные и неудобные для дальнейшего использования имена."
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dfWithLength\r\n    .select(\r\n        col(\"*\"),\r\n        (col(\"length\") - 1).as(\"l1\"),\r\n        (col(\"length\") % 2).alias(\"l2\"),\r\n        (col(\"length\") * 3).name(\"l3\")\r\n    ).show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Также мы можем просто удалить колонку"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dfWithLength.drop(\"length\").show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Работа с пропусками\n\nНад spark.DataFrame доступен модуль `.na`. Он содержит несколько функций для работы с пропусками:\n1. `.na.drop(colnames: Seq[String])` -- выбрасывает все пропуски, в списке колонок\n2. `.na.fill(map: Map[String, Value])` -- маппит пропуски в конкретных колонках (ключ мапы) на соответствующее значение\n3. `.na.replace(colname: String, map: Map[Value, Value])` -- заменяет значения в колонке в соответствии с мапой.\n\n\nПосмотрим, сколько наблюдений останется, если удалить пропуски по колонке `confidence`."
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df.na.drop(Seq(\"confidence\")).count"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Попробуем заменить пропуски средним. Для начала нам нужно его посчитать и получить значение на драйвере."
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val meanArray: Array[Row] \u003d df.select(mean(col(\"confidence\"))).collect"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "После `collect` мы получаем массив `Row`, который является нетипизированной коллекцией. Поэтому простое взятие элемента будет иметь тип `Any`, что может привести к неприятным последствиям."
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "meanArray(0)(0)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Давайте скастим его в `Double`"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val mn \u003d meanArray(0)(0).asInstanceOf[Double]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "И заполним пропуски, используя метод `.na.fill`"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val dfWithMean \u003d df.na.fill(Map(\n    \"confidence\" -\u003e mn\n))\n\ndfWithMean.show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Также в условии указано, что события `OPEN` и `CLICK` несут одинаковый смысл, поэтому переименуем `OPEN` в `CLICK` используя `.na.replace`"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val dfWithClick \u003d dfWithMean.na.replace(\"event\", Map(\"OPEN\" -\u003e \"CLICK\"))\ndfWithClick.show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Агрегаты\n\n\nДавайте посмотрим сколько было каждого из событий"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dfWithClick\n    .groupBy(\"event\")\n    .count()\n    .orderBy(desc(\"count\"))\n    .show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Мы можем также считать и много разных агрегаций за раз по аналогии с pd.DataFrame"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val aggregates \u003d dfWithClick\r\n    .groupBy(\"event\")\r\n    .agg(\r\n        count(\"*\").as(\"count\"),\r\n        min(col(\"confidence\")).as(\"min_confidence\"),\r\n        mean(col(\"confidence\")).as(\"mean_confidence\"),\r\n        max(col(\"confidence\")).as(\"max_confidence\")\r\n    )\r\n    .orderBy(desc(\"count\"))\r\n\r\naggregates.show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Оконные функции\n\nПеред тем как считать статистики, давайте выделим отдельные пользовательские сессии и проведем дедупликацию событий.\n\nСессией мы будем считать набор событий, между каждым последовательным из которых было меньше 10 минут (600 секунд). Тогда добавить индекс сессии мы можем по следующему пайплайну\n\n1. Взять лаг по времени для каждлого пользователя\n2. Заполнить нулями полученные пропуски\n3. Посчитать дельту по времени между двумя последовательными событиями пользователя\n4. Сравнить дельту с 600 секундами. Если больше, то считаем, что началась новая сессия\n5. Считаем кумулятивную сумму по индикаторам начала сессий"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.expressions.Window\n\nval userWindow \u003d Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n\nval dfWithLag \u003d dfWithClick.withColumn(\n    \"lag_timestamp\", lag(col(\"timestamp\"), 1).over(userWindow)\n    ).na.fill(Map(\"lag_timestamp\" -\u003e 0))"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dfWithLag.show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Находим индикаторы начала сессий"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val dfWithSessionStart \u003d dfWithLag.withColumn(\n    \"session_start\", (col(\"timestamp\") - col(\"lag_timestamp\") \u003e 600).cast(\"int\")\n)\n\ndfWithSessionStart.show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Считаем итоговый индекс сессии. Чтобы посчитать кумулятивную сумму, нам необходимо взять сумму по всем предыдущим строкам пользователя. Для этого мы можем в окне явно указать необходимые строки для агрегации через `rowsBetween`. Для использования всех предыдущих элементов можно использовать `Window.unboundedPreceding`."
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val sessionWindow \u003d Window.partitionBy(\"user_id\").orderBy(\"timestamp\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n\nval dfWithSession \u003d dfWithSessionStart.withColumn(\"session_id\", sum(col(\"session_start\")).over(sessionWindow))\n\ndfWithSession.show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n## User Defined Functions\n\nПо условию, мы проводили аб-тест между двумя моделями. Вариация для пользователя выбирается по через остаток от деления хэша с солью от id пользователя.  В качестве хэша взять MurmurHash3.\n\nТакого метода в DataFrame API по дефолту нет, поэтмоу может расширить его, используя UDF.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import scala.util.hashing.MurmurHash3\n\nval defineGroup \u003d udf((userId: Int) \u003d\u003e {\n    val hash \u003d Math.floorMod(MurmurHash3.stringHash(s\"${userId}|salt\"), 100)\n    if(hash \u003c 50) {\n        \"default\"\n    } else \"model\"\n})"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val dfWithGroup \u003d dfWithSession.withColumn(\n    \"group\", defineGroup(col(\"user_id\"))\n).cache()\n\ndfWithGroup.show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Посчитаем количество действий по разным группам"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val pivoted \u003d dfWithGroup\n    .groupBy(\"group\")\n    .pivot(\"event\")\n    .count()\n\npivoted.show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Сравним конверсии"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "pivoted.select(col(\"group\"), round(col(\"CLICK\") / col(\"SHOW\"), 4).as(\"clickRate\"), round(col(\"HIDE\") / col(\"SHOW\"), 4).as(\"hideRate\")).show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Видно, что на сырых событиях модель работает лучше. Давайте также дедуплицируем их по сессиям и также проверим конверсии"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val sessionDeduplicated \u003d dfWithGroup.groupBy(\"group\", \"user_id\", \"item_id\", \"event\", \"session_id\").agg(min(\"timestamp\").as(\"timestamp\")).cache()\n\nsessionDeduplicated\n    .groupBy(\"group\")\n    .pivot(\"event\")\n    .count()\n    .select(col(\"group\"), round(col(\"CLICK\") / col(\"SHOW\"), 4).as(\"clickRate\"), round(col(\"HIDE\") / col(\"SHOW\"), 4).as(\"hideRate\")).show"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Мы ответили на вопрос о том, что новая модель работает лучше и в терминах кликов, и в терминах скрытий баннера (на самом деле хорошо бы посчитать доверительный интервал, но вы уже можете это сделать сами) )."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Визуализация\n\nДавайте визуализируем динамику конверсий, чтобы ответить на остальные вопросы"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// давайте агрегируем количества по 12 часов\nval dfWithHour \u003d sessionDeduplicated.withColumn(\"hour\", floor(col(\"timestamp\") / 43200))\nval pivotedDynamics \u003d dfWithHour\n    .groupBy(\"group\", \"hour\")\n    .pivot(\"event\")\n    .count()\n    \n// посчитаем кумулятивные счетчики по времени\nval cumStatWindow \u003d Window.partitionBy(\"group\").orderBy(\"hour\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n\nval dynamicStats \u003d pivotedDynamics.na.fill(Map(\n    \"CLICK\" -\u003e 0,\n    \"HIDE\" -\u003e 0,\n    \"SHOW\" -\u003e 0\n))\n.withColumn(\"cumClicks\", sum(col(\"CLICK\")).over(cumStatWindow))\n.withColumn(\"cumHides\", sum(col(\"HIDE\")).over(cumStatWindow))\n.withColumn(\"cumShows\", sum(col(\"SHOW\")).over(cumStatWindow))\n.select(\n    col(\"group\"),\n    col(\"hour\"),\n    round(col(\"cumClicks\") / col(\"cumShows\"), 4).as(\"cumClickRate\"),\n    round(col(\"cumHides\") / col(\"cumShows\"), 4).as(\"cumHideRate\")\n)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\nПостроим графики, используя стандартные средства Zeppelin.\n\n\nВидно, что новая модель примерно через месяц начинает в общем работать лучше, чем исходная."
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(dynamicStats)"
    }
  ]
}